# MacStudioCluster - GPT-OSS-120B to MLX

Dieses Repository enthÃ¤lt Tools und Anleitungen fÃ¼r die Konvertierung von GPT-OSS-120B zu MLX-Format fÃ¼r optimale Performance auf Apple Silicon.

## ğŸ¯ Projekt-Ãœbersicht

- **Face Tagging System** mit MLX-Optimierung
- **GPT-OSS-120B Integration** fÃ¼r semantische Annotation
- **exo Framework** fÃ¼r verteilte AI-Inference
- **MLX-Monitoring Tools** fÃ¼r Performance-Tracking

## ğŸ“ Projekt-Struktur

```
MacStudioCluster/
â”œâ”€â”€ face_tag_software.md          # Software-Architektur Dokumentation
â”œâ”€â”€ face_tag_hardware.md           # Hardware-Architektur Dokumentation
â”œâ”€â”€ GPT_OSS_120B_ANLEITUNG.md     # GPT-OSS-120B Setup & Konvertierung
â”œâ”€â”€ mlx_monitor.py                 # MLX Performance Monitoring Tool
â”œâ”€â”€ exo/                           # exo Framework fÃ¼r verteilte Inference
â”‚   â”œâ”€â”€ MODELLE_HINZUFUEGEN.md    # Anleitung: Modelle zu exo hinzufÃ¼gen
â”‚   â””â”€â”€ add_model_example.py      # Script zum HinzufÃ¼gen von Modellen
â”œâ”€â”€ gpt-oss-120b/                  # GPT-OSS-120B Modell (nur Config, keine Weights)
â””â”€â”€ project_sizing_report.md      # Projekt-Sizing fÃ¼r 400k Fotos
```

## ğŸš€ Quick Start

### 1. MLX Environment Setup

```bash
# Virtual Environment erstellen
python3 -m venv mlx_env
source mlx_env/bin/activate

# MLX installieren
pip install mlx mlx-lm
```

### 2. GPT-OSS-120B zu MLX konvertieren

```python
from mlx_lm import convert

convert(
    hf_path="gpt-oss-120b/original/",
    mlx_path="./mlx_models/gpt-oss-120b-4bit",
    quantize=True,
    q_group_size=64
)
```

### 3. MLX Monitoring

```bash
source mlx_env/bin/activate
python mlx_monitor.py
```

## ğŸ“š Dokumentation

- **[GPT-OSS-120B Anleitung](./GPT_OSS_120B_ANLEITUNG.md)** - VollstÃ¤ndige Anleitung zur Konvertierung und Verwendung
- **[Face Tagging Software](./face_tag_software.md)** - Detaillierte Software-Architektur
- **[Face Tagging Hardware](./face_tag_hardware.md)** - Hardware-Architektur fÃ¼r Mac Studio Cluster
- **[exo Modelle hinzufÃ¼gen](./exo/MODELLE_HINZUFUEGEN.md)** - Anleitung zum HinzufÃ¼gen von Modellen zu exo

## ğŸ› ï¸ Features

### MLX-Optimierung
- âœ… Native Apple Silicon Performance
- âœ… Unified Memory Architecture
- âœ… 2-4x schneller als PyTorch
- âœ… Quantisierung (4-bit/8-bit)

### GPT-OSS-120B Integration
- âœ… PyTorch Support (MPS)
- âœ… MLX-Konvertierung
- âœ… exo-Integration fÃ¼r Multi-Device
- âœ… Face Tagging Pipeline Integration

### exo Framework
- âœ… Verteile AI-Inference Ã¼ber mehrere GerÃ¤te
- âœ… Automatische Device-Discovery
- âœ… ChatGPT-kompatible API
- âœ… MLX & tinygrad Support

## ğŸ“Š Performance

**MLX vs PyTorch auf Apple Silicon:**

| Komponente | PyTorch | MLX | Verbesserung |
|------------|---------|-----|--------------|
| Face Detection | 1.5s | 0.5-0.8s | **~2x** |
| Embedding | 0.3s | 0.1-0.15s | **~2x** |
| LLM Inference (70B) | 15-20 tok/s | 30-40 tok/s | **~2x** |

## ğŸ”§ Requirements

- **Python:** 3.10+
- **macOS:** 12.3+ (fÃ¼r MPS Support)
- **RAM:** 64GB+ empfohlen fÃ¼r GPT-OSS-120B
- **Hardware:** Apple Silicon (M1/M2/M3/M4)

## ğŸ“ License

Siehe LICENSE Dateien in den jeweiligen Unterprojekten.

## ğŸ¤ Contributing

BeitrÃ¤ge sind willkommen! Bitte erstelle ein Issue oder Pull Request.

## ğŸ“§ Kontakt

FÃ¼r Fragen oder Support, Ã¶ffne bitte ein Issue auf GitHub.

---

**Hinweis:** Die Modell-Gewichte (`.safetensors` Dateien) sind nicht im Repository enthalten, da sie zu groÃŸ sind. Bitte lade sie separat von Hugging Face herunter.
